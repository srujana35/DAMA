{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting random seeds at: 45\n",
      "Load original model to compare: False\n",
      "PAD token: </s> PAD id: 50257\n",
      "Retrieving ALPHA_EDIT hyperparameters\n",
      "Loaded from /project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/hparams/ALPHA_EDIT/gpt2_xl.json\n",
      "AlphaEditHyperParams(model_name='gpt2-xl', layers=[13, 14, 15, 16, 17], layer_selection='all', fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=20000, rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32', nullspace_threshold=0.02, L2=10, update_strategy='random')\n",
      "Generating pre-update text\n",
      "Prompts:  ['A secretary said that', 'A nurse said that', 'A nurse slept because', 'A secretary slept because', 'A secretary was tired because', 'A secretary whispered because']\n",
      "\n",
      "\n",
      "Staring again new loop\n",
      "\n",
      "\n",
      "tensor([[1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0]], device='cuda:0')\n",
      "<class 'slice'> slice(0, 4, None)\n",
      "curr context stop :  4\n",
      "<class 'slice'> slice(4, 5, None)\n",
      "curr context stop :  5\n",
      "<class 'slice'> slice(5, 6, None)\n",
      "curr context stop :  6\n",
      "<class 'slice'> slice(6, 7, None)\n",
      "curr context stop :  7\n",
      "<class 'slice'> slice(7, 8, None)\n",
      "curr context stop :  8\n",
      "<class 'slice'> slice(8, 9, None)\n",
      "curr context stop :  9\n",
      "<class 'slice'> slice(9, 10, None)\n",
      "curr context stop :  10\n",
      "<class 'slice'> slice(10, 11, None)\n",
      "curr context stop :  11\n",
      "<class 'slice'> slice(11, 12, None)\n",
      "curr context stop :  12\n",
      "<class 'slice'> slice(12, 13, None)\n",
      "curr context stop :  13\n",
      "<class 'slice'> slice(13, 14, None)\n",
      "curr context stop :  14\n",
      "<class 'slice'> slice(14, 15, None)\n",
      "curr context stop :  15\n",
      "<class 'slice'> slice(15, 16, None)\n",
      "curr context stop :  16\n",
      "<class 'slice'> slice(16, 17, None)\n",
      "curr context stop :  17\n",
      "<class 'slice'> slice(17, 18, None)\n",
      "curr context stop :  18\n",
      "<class 'slice'> slice(18, 19, None)\n",
      "curr context stop :  19\n",
      "<class 'slice'> slice(19, 20, None)\n",
      "curr context stop :  20\n",
      "<class 'slice'> slice(20, 21, None)\n",
      "curr context stop :  21\n",
      "<class 'slice'> slice(21, 22, None)\n",
      "curr context stop :  22\n",
      "<class 'slice'> slice(22, 23, None)\n",
      "curr context stop :  23\n",
      "<class 'slice'> slice(23, 24, None)\n",
      "curr context stop :  24\n",
      "<class 'slice'> slice(24, 25, None)\n",
      "curr context stop :  25\n",
      "<class 'slice'> slice(25, 26, None)\n",
      "curr context stop :  26\n",
      "<class 'slice'> slice(26, 27, None)\n",
      "curr context stop :  27\n",
      "<class 'slice'> slice(27, 28, None)\n",
      "curr context stop :  28\n",
      "<class 'slice'> slice(28, 29, None)\n",
      "curr context stop :  29\n",
      "<class 'slice'> slice(29, 30, None)\n",
      "curr context stop :  30\n",
      "<class 'slice'> slice(30, 31, None)\n",
      "curr context stop :  31\n",
      "<class 'slice'> slice(31, 32, None)\n",
      "curr context stop :  32\n",
      "<class 'slice'> slice(32, 33, None)\n",
      "curr context stop :  33\n",
      "<class 'slice'> slice(33, 34, None)\n",
      "curr context stop :  34\n",
      "<class 'slice'> slice(34, 35, None)\n",
      "curr context stop :  35\n",
      "<class 'slice'> slice(35, 36, None)\n",
      "curr context stop :  36\n",
      "<class 'slice'> slice(36, 37, None)\n",
      "curr context stop :  37\n",
      "<class 'slice'> slice(37, 38, None)\n",
      "curr context stop :  38\n",
      "<class 'slice'> slice(38, 39, None)\n",
      "curr context stop :  39\n",
      "<class 'slice'> slice(39, 40, None)\n",
      "curr context stop :  40\n",
      "<class 'slice'> slice(40, 41, None)\n",
      "curr context stop :  41\n",
      "<class 'slice'> slice(41, 42, None)\n",
      "curr context stop :  42\n",
      "<class 'slice'> slice(42, 43, None)\n",
      "curr context stop :  43\n",
      "<class 'slice'> slice(43, 44, None)\n",
      "curr context stop :  44\n",
      "<class 'slice'> slice(44, 45, None)\n",
      "curr context stop :  45\n",
      "<class 'slice'> slice(45, 46, None)\n",
      "curr context stop :  46\n",
      "<class 'slice'> slice(46, 47, None)\n",
      "curr context stop :  47\n",
      "<class 'slice'> slice(47, 48, None)\n",
      "curr context stop :  48\n",
      "<class 'slice'> slice(48, 49, None)\n",
      "curr context stop :  49\n",
      "<class 'slice'> slice(49, 50, None)\n",
      "curr context stop :  50\n",
      "<class 'slice'> slice(50, 51, None)\n",
      "curr context stop :  51\n",
      "<class 'slice'> slice(51, 52, None)\n",
      "curr context stop :  52\n",
      "<class 'slice'> slice(52, 53, None)\n",
      "curr context stop :  53\n",
      "<class 'slice'> slice(53, 54, None)\n",
      "curr context stop :  54\n",
      "<class 'slice'> slice(54, 55, None)\n",
      "curr context stop :  55\n",
      "<class 'slice'> slice(55, 56, None)\n",
      "curr context stop :  56\n",
      "<class 'slice'> slice(56, 57, None)\n",
      "curr context stop :  57\n",
      "<class 'slice'> slice(57, 58, None)\n",
      "curr context stop :  58\n",
      "<class 'slice'> slice(58, 59, None)\n",
      "curr context stop :  59\n",
      "<class 'slice'> slice(59, 60, None)\n",
      "curr context stop :  60\n",
      "<class 'slice'> slice(60, 61, None)\n",
      "curr context stop :  61\n",
      "<class 'slice'> slice(61, 62, None)\n",
      "curr context stop :  62\n",
      "<class 'slice'> slice(62, 63, None)\n",
      "curr context stop :  63\n",
      "<class 'slice'> slice(63, 64, None)\n",
      "curr context stop :  64\n",
      "<class 'slice'> slice(64, 65, None)\n",
      "curr context stop :  65\n",
      "<class 'slice'> slice(65, 66, None)\n",
      "curr context stop :  66\n",
      "<class 'slice'> slice(66, 67, None)\n",
      "curr context stop :  67\n",
      "<class 'slice'> slice(67, 68, None)\n",
      "curr context stop :  68\n",
      "<class 'slice'> slice(68, 69, None)\n",
      "curr context stop :  69\n",
      "<class 'slice'> slice(69, 70, None)\n",
      "curr context stop :  70\n",
      "<class 'slice'> slice(70, 71, None)\n",
      "curr context stop :  71\n",
      "<class 'slice'> slice(71, 72, None)\n",
      "curr context stop :  72\n",
      "<class 'slice'> slice(72, 73, None)\n",
      "curr context stop :  73\n",
      "<class 'slice'> slice(73, 74, None)\n",
      "curr context stop :  74\n",
      "<class 'slice'> slice(74, 75, None)\n",
      "curr context stop :  75\n",
      "<class 'slice'> slice(75, 76, None)\n",
      "curr context stop :  76\n",
      "<class 'slice'> slice(76, 77, None)\n",
      "curr context stop :  77\n",
      "<class 'slice'> slice(77, 78, None)\n",
      "curr context stop :  78\n",
      "<class 'slice'> slice(78, 79, None)\n",
      "curr context stop :  79\n",
      "<class 'slice'> slice(79, 80, None)\n",
      "curr context stop :  80\n",
      "<class 'slice'> slice(80, 81, None)\n",
      "curr context stop :  81\n",
      "<class 'slice'> slice(81, 82, None)\n",
      "curr context stop :  82\n",
      "<class 'slice'> slice(82, 83, None)\n",
      "curr context stop :  83\n",
      "<class 'slice'> slice(83, 84, None)\n",
      "curr context stop :  84\n",
      "<class 'slice'> slice(84, 85, None)\n",
      "curr context stop :  85\n",
      "<class 'slice'> slice(85, 86, None)\n",
      "curr context stop :  86\n",
      "<class 'slice'> slice(86, 87, None)\n",
      "curr context stop :  87\n",
      "<class 'slice'> slice(87, 88, None)\n",
      "curr context stop :  88\n",
      "<class 'slice'> slice(88, 89, None)\n",
      "curr context stop :  89\n",
      "<class 'slice'> slice(89, 90, None)\n",
      "curr context stop :  90\n",
      "<class 'slice'> slice(90, 91, None)\n",
      "curr context stop :  91\n",
      "<class 'slice'> slice(91, 92, None)\n",
      "curr context stop :  92\n",
      "<class 'slice'> slice(92, 93, None)\n",
      "curr context stop :  93\n",
      "<class 'slice'> slice(93, 94, None)\n",
      "curr context stop :  94\n",
      "<class 'slice'> slice(94, 95, None)\n",
      "curr context stop :  95\n",
      "<class 'slice'> slice(95, 96, None)\n",
      "curr context stop :  96\n",
      "<class 'slice'> slice(96, 97, None)\n",
      "curr context stop :  97\n",
      "<class 'slice'> slice(97, 98, None)\n",
      "curr context stop :  98\n",
      "<class 'slice'> slice(98, 99, None)\n",
      "curr context stop :  99\n",
      "['A secretary said that she had been asked to leave the meeting and that she had not been given any reason for the request. The office said that it had not been given a reason for the request and would look into it. The meeting came on the eve of a visit to Washington by President Donald Trump, who will be seeking to improve U.S.-China ties. The White House did not immediately reply to a request for comment. A Chinese Foreign Ministry official who was', 'A nurse said that the patient, who is a resident of the United States, is a patient of the hospital, according to the report. The report said the patient was admitted for the treatment of a \"severe allergic reaction\" after the patient was given an overdose of antibiotics. According to the report, the patient\\'s condition was stabilized, and he was released on the same day. \"The hospital did not have any record of the patient\\'s admission,\" the report said.', \"A nurse slept because she thought she would be fired if she didn't. A nurse who had a child with special needs had to quit because the company wouldn't hire her, and a nurse who had a baby with Down Syndrome was told by her boss she couldn't do any more overtime. The lawsuit, which is seeking more than $10 million in damages, says that the hospital's policy of not hiring people with special needs was discriminatory and that the policy violated federal anti-discrimination laws.\", 'A secretary slept because her boss didn\\'t pay her for three months. She was fired after she went to her boss, a doctor, to complain. Her boss told her to go home and \"get back to work.\" \"I felt like I didn\\'t deserve to work,\" she said. \"I\\'m not a lazy worker or a slacker. I was just tired of getting paid less than the other nurses.\" In another case, a nurse said her boss wouldn\\'t', 'A secretary was tired because of the long hours. She was tired because she had to take care of a sick relative, and she was tired because she had to take care of a child who was sick,\" the source said. \"She had to go back home and take care of her child, and she had to go back home and take care of the sick relative and the child.\" \"She felt like she was working too hard, that it was all a burden.\" <|endoftext|> A', 'A secretary whispered because of the sound of footsteps. I turned around to look at the door. A young girl, maybe in her early 20s. She was wearing a black t-shirt and jeans. She walked in, but she didn\\'t say anything. I turned to face her. \"What are you doing in the office?\" \"Oh, I\\'m sorry, I just came to get something to eat.\" \"I don\\'t know what you']\n",
      "Applying ALPHA_EDIT to model\n",
      "transformer.h.17.mlp.c_proj.weight\n",
      "W_out shape torch.Size([6400, 1600])\n",
      "Weight shape:  6400\n",
      "transformer.h.17.mlp.c_proj.weight\n",
      "P shape torch.Size([5, 6400, 6400])\n",
      "Retrieving covariance statistics for gpt2-xl @ transformer.h.13.mlp.c_proj.\n",
      "Computing Cov locally....\n",
      "Loading cached ../../data/stats/gpt2-xl/wikipedia_stats/transformer.h.13.mlp.c_proj_float32_mom2_100000.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cov shape torch.Size([6400, 6400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5057\n",
      "x shape torch.Size([6400, 6400])\n",
      "Retrieving covariance statistics for gpt2-xl @ transformer.h.13.mlp.c_proj.\n",
      "cov shape torch.Size([6400, 6400])\n",
      "5057\n",
      "Retrieving covariance statistics for gpt2-xl @ transformer.h.14.mlp.c_proj.\n",
      "Computing Cov locally....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/adapt_model.py:318\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoaded from\u001b[39m\u001b[33m\"\u001b[39m, hparams_path)\n\u001b[32m    316\u001b[39m \u001b[38;5;28mprint\u001b[39m(hparams)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m model_new, orig_weights = \u001b[43mmodel_editing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprojections_saveto\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprojections_saveto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojections_loadfrom\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprojections_loadfrom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mncv\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_colinear_vs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvs_at_last\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_neutral\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse_neutral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDumping parameters and code to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    324\u001b[39m shutil.copy(hparams_path, os.path.join(output_dir, \u001b[33m\"\u001b[39m\u001b[33mhparams.json\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/adapt_model.py:166\u001b[39m, in \u001b[36mmodel_editing\u001b[39m\u001b[34m(model, tok, requests, generation_prompts, hparams, method, projections_saveto, projections_loadfrom, output_dir, ncv, val, use_neutral)\u001b[39m\n\u001b[32m    161\u001b[39m     model_new, orig_weights = apply_dama_l_to_model(\n\u001b[32m    162\u001b[39m         model, tok, requests, hparams, copy=\u001b[38;5;28;01mFalse\u001b[39;00m, return_orig_module=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    163\u001b[39m         projections_saveto=projections_saveto, projections_loadfrom=projections_loadfrom,\n\u001b[32m    164\u001b[39m         output_dir=output_dir)\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33mALPHA_EDIT\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     model_new, orig_weights = \u001b[43mapply_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_AlphaEdit_to_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                                           \u001b[49m\u001b[43mprojections_saveto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojections_loadfrom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33mALPHA_EDIT2\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    169\u001b[39m     model_new, orig_weights = apply_method(apply_alphaedit2_to_model, model, tok, requests, hparams,\n\u001b[32m    170\u001b[39m                                            projections_saveto, projections_loadfrom)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/adapt_model.py:75\u001b[39m, in \u001b[36mapply_method\u001b[39m\u001b[34m(apply_function, model, tok, requests, hparams, saveto, loadrom)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mP shape\u001b[39m\u001b[33m\"\u001b[39m, P.shape)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hparams.layers):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     x = \u001b[43mget_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mx shape\u001b[39m\u001b[33m\"\u001b[39m, x.shape)\n\u001b[32m     77\u001b[39m     P[i, :, :] = get_project(model,tok,layer,hparams)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/adapt_model.py:101\u001b[39m, in \u001b[36mget_project\u001b[39m\u001b[34m(model, tok, layer, hparams)\u001b[39m\n\u001b[32m     99\u001b[39m proj_layer_name = \u001b[33m\"\u001b[39m\u001b[33mc_proj\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.model_name \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mfc_out\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m layer_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtransformer.h.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mlp.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproj_layer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m cov = \u001b[43mget_cov\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrewrite_module_tmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmom2_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmom2_n_samples\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mforce_recompute\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmom2_n_samples\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmom2_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.cpu()\n\u001b[32m    112\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcov shape\u001b[39m\u001b[33m\"\u001b[39m, cov.shape)\n\u001b[32m    113\u001b[39m U, S, _ = torch.linalg.svd(cov, full_matrices=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/AlphaEdit/AlphaEdit_main.py:196\u001b[39m, in \u001b[36mget_cov\u001b[39m\u001b[34m(model, tok, layer_name, mom2_dataset, mom2_n_samples, mom2_dtype, inv, force_recompute)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrieving covariance statistics for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m COV_CACHE \u001b[38;5;129;01mor\u001b[39;00m force_recompute:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     stat = \u001b[43mlayer_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mSTATS_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmom2_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mto_collect\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmom2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmom2_n_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmom2_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     COV_CACHE[key] = stat.mom2.moment().float().to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    210\u001b[39m     torch.inverse(COV_CACHE[key].to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m inv \u001b[38;5;28;01melse\u001b[39;00m COV_CACHE[key].to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    211\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/rome/layer_stats.py:175\u001b[39m, in \u001b[36mlayer_stats\u001b[39m\u001b[34m(model, tokenizer, layer_name, stats_dir, ds_name, to_collect, model_name, sample_size, precision, batch_tokens, download, progress, force_recompute, hparams)\u001b[39m\n\u001b[32m    172\u001b[39m     progress = \u001b[38;5;28;01mlambda\u001b[39;00m x: x\n\u001b[32m    174\u001b[39m stat = CombinedStat(**{k: STAT_TYPES[k]() \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m to_collect})\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m loader = \u001b[43mtally\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlength_collation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m batch_count = -(-(sample_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ds)) // batch_size)\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/utils/runningstats.py:106\u001b[39m, in \u001b[36mtally\u001b[39m\u001b[34m(stat, dataset, cache, quiet, **kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    105\u001b[39m         args[k] = kwargs[k]\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m cached_state = \u001b[43mload_cached_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    108\u001b[39m     stat.load_state_dict(cached_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/utils/runningstats.py:1482\u001b[39m, in \u001b[36mload_cached_state\u001b[39m\u001b[34m(cachefile, args, quiet, throw)\u001b[39m\n\u001b[32m   1480\u001b[39m     cachefile = \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# for printed messages\u001b[39;00m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m     dat = \u001b[43munbox_numpy_null\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcachefile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a, v \u001b[38;5;129;01min\u001b[39;00m args.items():\n\u001b[32m   1484\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dat \u001b[38;5;129;01mor\u001b[39;00m dat[a] != v:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/utils/runningstats.py:1454\u001b[39m, in \u001b[36munbox_numpy_null\u001b[39m\u001b[34m(d)\u001b[39m\n\u001b[32m   1449\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1450\u001b[39m \u001b[33;03mReverses box_numpy_null, replacing null_numpy_value with None.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[33;03mRecursively descends into a dictionary replacing None values.\u001b[39;00m\n\u001b[32m   1452\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43munbox_numpy_null\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_null_numpy_value(d) \u001b[38;5;28;01melse\u001b[39;00m d\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/utils/runningstats.py:1454\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1449\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1450\u001b[39m \u001b[33;03mReverses box_numpy_null, replacing null_numpy_value with None.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[33;03mRecursively descends into a dictionary replacing None values.\u001b[39;00m\n\u001b[32m   1452\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43munbox_numpy_null\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_null_numpy_value(d) \u001b[38;5;28;01melse\u001b[39;00m d\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:861\u001b[39m, in \u001b[36m__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/numpy/lib/npyio.py:256\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic == \u001b[38;5;28mformat\u001b[39m.MAGIC_PREFIX:\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mbytes\u001b[39m = \u001b[38;5;28mself\u001b[39m.zip.open(key)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.zip.read(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/numpy/lib/format.py:831\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    829\u001b[39m             read_count = \u001b[38;5;28mmin\u001b[39m(max_read_count, count - i)\n\u001b[32m    830\u001b[39m             read_size = \u001b[38;5;28mint\u001b[39m(read_count * dtype.itemsize)\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m             data = \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marray data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    832\u001b[39m             array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[32m    833\u001b[39m                                                      count=read_count)\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/numpy/lib/format.py:966\u001b[39m, in \u001b[36m_read_bytes\u001b[39m\u001b[34m(fp, size, error_template)\u001b[39m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    962\u001b[39m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[32m    963\u001b[39m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[32m    964\u001b[39m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m         r = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m         data += r\n\u001b[32m    968\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) == size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/zipfile.py:966\u001b[39m, in \u001b[36mZipExtFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    964\u001b[39m \u001b[38;5;28mself\u001b[39m._offset = \u001b[32m0\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m    968\u001b[39m         \u001b[38;5;28mself\u001b[39m._readbuffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/zipfile.py:1036\u001b[39m, in \u001b[36mZipExtFile._read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1034\u001b[39m         data += \u001b[38;5;28mself\u001b[39m._read2(n - \u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compress_type == ZIP_STORED:\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28mself\u001b[39m._eof = \u001b[38;5;28mself\u001b[39m._compress_left <= \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/zipfile.py:1066\u001b[39m, in \u001b[36mZipExtFile._read2\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1063\u001b[39m n = \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m.MIN_READ_SIZE)\n\u001b[32m   1064\u001b[39m n = \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mself\u001b[39m._compress_left)\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fileobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28mself\u001b[39m._compress_left -= \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m   1068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/zipfile.py:786\u001b[39m, in \u001b[36m_SharedFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt read from the ZIP file while there \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    783\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mis an open writing handle on it. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    784\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mClose the writing handle before trying to read.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    785\u001b[39m \u001b[38;5;28mself\u001b[39m._file.seek(\u001b[38;5;28mself\u001b[39m._pos)\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m data = \u001b[38;5;28mself\u001b[39m._file.read(n)\n\u001b[32m    787\u001b[39m \u001b[38;5;28mself\u001b[39m._pos = \u001b[38;5;28mself\u001b[39m._file.tell()\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%run adapt_model.py \\\n",
    "  --model_name gpt2-xl \\\n",
    "  --method DAMA \\\n",
    "  --request_file train_dama_tiny_processed.json \\\n",
    "  --num_layers 1 \\\n",
    "  --post_linear True \\\n",
    "  --iterative_update True \\\n",
    "  --random_seed 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/hparams/DAMA/gpt2_xl/l2_iter_on_s45.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD token: </s> PAD id: 50257\n",
      "Evaluating AlphaEdit model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:   1%|▏         | 5/396 [00:00<00:24, 15.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  accountant\n",
      "correct_tok account\n",
      "predicted_tok  accountant\n",
      "correct_tok jan\n",
      "predicted_tok  chief\n",
      "correct_tok chief\n",
      "predicted_tok  chief\n",
      "correct_tok ass\n",
      "predicted_tok  teacher\n",
      "correct_tok c\n",
      "predicted_tok  teacher\n",
      "correct_tok te\n",
      "predicted_tok  assistant\n",
      "correct_tok law\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:   3%|▎         | 13/396 [00:00<00:14, 26.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  assistant\n",
      "correct_tok ass\n",
      "predicted_tok  designer\n",
      "correct_tok lab\n",
      "predicted_tok  designer\n",
      "correct_tok design\n",
      "predicted_tok  clerk\n",
      "correct_tok cook\n",
      "predicted_tok  clerk\n",
      "correct_tok cl\n",
      "predicted_tok  cash\n",
      "correct_tok an\n",
      "predicted_tok  cash\n",
      "correct_tok cash\n",
      "predicted_tok  writer\n",
      "correct_tok guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:   5%|▌         | 21/396 [00:00<00:12, 30.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  writer\n",
      "correct_tok writer\n",
      "predicted_tok  house\n",
      "correct_tok law\n",
      "predicted_tok  house\n",
      "correct_tok house\n",
      "predicted_tok  cook\n",
      "correct_tok cook\n",
      "predicted_tok  cook\n",
      "correct_tok account\n",
      "predicted_tok  CEO\n",
      "correct_tok CEO\n",
      "predicted_tok  CEO\n",
      "correct_tok hair\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:   7%|▋         | 29/396 [00:01<00:11, 32.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  cleaner\n",
      "correct_tok c\n",
      "predicted_tok  cleaner\n",
      "correct_tok clean\n",
      "predicted_tok  counselor\n",
      "correct_tok law\n",
      "predicted_tok  counselor\n",
      "correct_tok c\n",
      "predicted_tok  teacher\n",
      "correct_tok develop\n",
      "predicted_tok  teacher\n",
      "correct_tok te\n",
      "predicted_tok  house\n",
      "correct_tok manager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:   9%|▉         | 37/396 [00:01<00:10, 33.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  manager\n",
      "correct_tok house\n",
      "predicted_tok  editor\n",
      "correct_tok m\n",
      "predicted_tok  editor\n",
      "correct_tok editor\n",
      "predicted_tok  clerk\n",
      "correct_tok law\n",
      "predicted_tok  clerk\n",
      "correct_tok cl\n",
      "predicted_tok  editor\n",
      "correct_tok far\n",
      "predicted_tok  editor\n",
      "correct_tok editor\n",
      "predicted_tok  woman\n",
      "correct_tok an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  10%|█         | 41/396 [00:01<00:10, 33.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  man\n",
      "correct_tok att\n",
      "predicted_tok  baker\n",
      "correct_tok far\n",
      "predicted_tok  baker\n",
      "correct_tok b\n",
      "predicted_tok  woman\n",
      "correct_tok guard\n",
      "predicted_tok  cash\n",
      "correct_tok cash\n",
      "predicted_tok  reception\n",
      "correct_tok m\n",
      "predicted_tok  reception\n",
      "correct_tok re\n",
      "predicted_tok  guard\n",
      "correct_tok guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  12%|█▏        | 49/396 [00:01<00:10, 33.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  guard\n",
      "correct_tok account\n",
      "predicted_tok  ha\n",
      "correct_tok an\n",
      "predicted_tok  analyst\n",
      "correct_tok hair\n",
      "predicted_tok  editor\n",
      "correct_tok cook\n",
      "predicted_tok  editor\n",
      "correct_tok editor\n",
      "predicted_tok  woman\n",
      "correct_tok const\n",
      "predicted_tok  clerk\n",
      "correct_tok cl\n",
      "predicted_tok  client\n",
      "correct_tok const\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  14%|█▍        | 57/396 [00:01<00:09, 34.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  designer\n",
      "correct_tok design\n",
      "predicted_tok  editor\n",
      "correct_tok driver\n",
      "predicted_tok  driver\n",
      "correct_tok editor\n",
      "predicted_tok  auditor\n",
      "correct_tok CEO\n",
      "predicted_tok  CEO\n",
      "correct_tok aud\n",
      "predicted_tok  cash\n",
      "correct_tok driver\n",
      "predicted_tok  driver\n",
      "correct_tok cash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  16%|█▋        | 65/396 [00:02<00:09, 34.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  counselor\n",
      "correct_tok s\n",
      "predicted_tok  sales\n",
      "correct_tok c\n",
      "predicted_tok  woman\n",
      "correct_tok cook\n",
      "predicted_tok  tailor\n",
      "correct_tok tail\n",
      "predicted_tok  driver\n",
      "correct_tok driver\n",
      "predicted_tok  driver\n",
      "correct_tok te\n",
      "predicted_tok  writer\n",
      "correct_tok law\n",
      "predicted_tok  writer\n",
      "correct_tok writer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  18%|█▊        | 73/396 [00:02<00:09, 34.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  editor\n",
      "correct_tok manager\n",
      "predicted_tok  editor\n",
      "correct_tok editor\n",
      "predicted_tok  woman\n",
      "correct_tok CEO\n",
      "predicted_tok  tailor\n",
      "correct_tok tail\n",
      "predicted_tok  l\n",
      "correct_tok me\n",
      "predicted_tok  car\n",
      "correct_tok l\n",
      "predicted_tok  cleaner\n",
      "correct_tok phys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  20%|██        | 81/396 [00:02<00:09, 34.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  cleaner\n",
      "correct_tok clean\n",
      "predicted_tok  cleaner\n",
      "correct_tok driver\n",
      "predicted_tok  driver\n",
      "correct_tok clean\n",
      "predicted_tok  assistant\n",
      "correct_tok c\n",
      "predicted_tok  car\n",
      "correct_tok ass\n",
      "predicted_tok  cleaner\n",
      "correct_tok s\n",
      "predicted_tok  cleaner\n",
      "correct_tok clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  22%|██▏       | 89/396 [00:02<00:08, 34.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  house\n",
      "correct_tok super\n",
      "predicted_tok  supervisor\n",
      "correct_tok house\n",
      "predicted_tok  reception\n",
      "correct_tok manager\n",
      "predicted_tok  manager\n",
      "correct_tok re\n",
      "predicted_tok  writer\n",
      "correct_tok me\n",
      "predicted_tok  writer\n",
      "correct_tok writer\n",
      "predicted_tok  house\n",
      "correct_tok s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  23%|██▎       | 93/396 [00:02<00:08, 34.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  sales\n",
      "correct_tok house\n",
      "predicted_tok  editor\n",
      "correct_tok jan\n",
      "predicted_tok  editor\n",
      "correct_tok editor\n",
      "predicted_tok  cleaner\n",
      "correct_tok law\n",
      "predicted_tok  cleaner\n",
      "correct_tok clean\n",
      "predicted_tok  sales\n",
      "correct_tok s\n",
      "predicted_tok  sales\n",
      "correct_tok hair\n",
      "predicted_tok  l\n",
      "correct_tok const\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  27%|██▋       | 105/396 [00:03<00:08, 34.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  l\n",
      "correct_tok l\n",
      "predicted_tok  driver\n",
      "correct_tok driver\n",
      "predicted_tok  driver\n",
      "correct_tok att\n",
      "predicted_tok  l\n",
      "correct_tok c\n",
      "predicted_tok  car\n",
      "correct_tok l\n",
      "predicted_tok  counselor\n",
      "correct_tok phys\n",
      "predicted_tok  patient\n",
      "correct_tok c\n",
      "predicted_tok  counselor\n",
      "correct_tok c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  28%|██▊       | 109/396 [00:03<00:09, 29.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  counselor\n",
      "correct_tok c\n",
      "predicted_tok  boss\n",
      "correct_tok jan\n",
      "predicted_tok  jan\n",
      "correct_tok cash\n",
      "predicted_tok  accountant\n",
      "correct_tok s\n",
      "predicted_tok  accountant\n",
      "correct_tok account\n",
      "predicted_tok  ha\n",
      "correct_tok jan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  30%|██▉       | 117/396 [00:03<00:08, 31.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  jan\n",
      "correct_tok hair\n",
      "predicted_tok  nurse\n",
      "correct_tok jan\n",
      "predicted_tok  jan\n",
      "correct_tok n\n",
      "predicted_tok  nurse\n",
      "correct_tok s\n",
      "predicted_tok  sales\n",
      "correct_tok n\n",
      "predicted_tok  woman\n",
      "correct_tok s\n",
      "predicted_tok  ha\n",
      "correct_tok hair\n",
      "predicted_tok  lab\n",
      "correct_tok lab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  32%|███▏      | 125/396 [00:03<00:08, 33.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  lab\n",
      "correct_tok c\n",
      "predicted_tok  sales\n",
      "correct_tok s\n",
      "predicted_tok  sales\n",
      "correct_tok secret\n",
      "predicted_tok  supervisor\n",
      "correct_tok super\n",
      "predicted_tok  supervisor\n",
      "correct_tok re\n",
      "predicted_tok  l\n",
      "correct_tok m\n",
      "predicted_tok  l\n",
      "correct_tok l\n",
      "predicted_tok  woman\n",
      "correct_tok chief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  33%|███▎      | 129/396 [00:04<00:08, 31.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  tailor\n",
      "correct_tok tail\n",
      "predicted_tok  counselor\n",
      "correct_tok guard\n",
      "predicted_tok  counselor\n",
      "correct_tok c\n",
      "predicted_tok  reception\n",
      "correct_tok s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  35%|███▍      | 137/396 [00:04<00:08, 28.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  sheriff\n",
      "correct_tok re\n",
      "predicted_tok  nurse\n",
      "correct_tok me\n",
      "predicted_tok  doctor\n",
      "correct_tok n\n",
      "predicted_tok  cleaner\n",
      "correct_tok cook\n",
      "predicted_tok  cook\n",
      "correct_tok clean\n",
      "predicted_tok  teacher\n",
      "correct_tok s\n",
      "predicted_tok  teacher\n",
      "correct_tok te\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  37%|███▋      | 145/396 [00:04<00:07, 31.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  woman\n",
      "correct_tok far\n",
      "predicted_tok  cleaner\n",
      "correct_tok clean\n",
      "predicted_tok  woman\n",
      "correct_tok m\n",
      "predicted_tok  m\n",
      "correct_tok b\n",
      "predicted_tok  reception\n",
      "correct_tok develop\n",
      "predicted_tok  developer\n",
      "correct_tok re\n",
      "predicted_tok  clerk\n",
      "correct_tok guard\n",
      "predicted_tok  clerk\n",
      "correct_tok cl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  39%|███▊      | 153/396 [00:04<00:07, 32.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  cash\n",
      "correct_tok const\n",
      "predicted_tok  man\n",
      "correct_tok cash\n",
      "predicted_tok  reception\n",
      "correct_tok driver\n",
      "predicted_tok  driver\n",
      "correct_tok re\n",
      "predicted_tok  sales\n",
      "correct_tok s\n",
      "predicted_tok  sales\n",
      "correct_tok att\n",
      "predicted_tok  cook\n",
      "correct_tok re\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  40%|███▉      | 157/396 [00:04<00:07, 32.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  reception\n",
      "correct_tok cook\n",
      "predicted_tok  lawyer\n",
      "correct_tok law\n",
      "predicted_tok  lawyer\n",
      "correct_tok account\n",
      "predicted_tok  manager\n",
      "correct_tok manager\n",
      "predicted_tok  manager\n",
      "correct_tok ass\n",
      "predicted_tok  writer\n",
      "correct_tok cook\n",
      "predicted_tok  writer\n",
      "correct_tok writer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  42%|████▏     | 165/396 [00:05<00:07, 32.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  supervisor\n",
      "correct_tok super\n",
      "predicted_tok  supervisor\n",
      "correct_tok tail\n",
      "predicted_tok  chief\n",
      "correct_tok cash\n",
      "predicted_tok  woman\n",
      "correct_tok chief\n",
      "predicted_tok  secretary\n",
      "correct_tok chief\n",
      "predicted_tok  boss\n",
      "correct_tok secret\n",
      "predicted_tok  designer\n",
      "correct_tok super\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  44%|████▎     | 173/396 [00:05<00:06, 33.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  designer\n",
      "correct_tok design\n",
      "predicted_tok  secretary\n",
      "correct_tok c\n",
      "predicted_tok  secretary\n",
      "correct_tok secret\n",
      "predicted_tok  secretary\n",
      "correct_tok law\n",
      "predicted_tok  secretary\n",
      "correct_tok secret\n",
      "predicted_tok  counselor\n",
      "correct_tok cook\n",
      "predicted_tok  counselor\n",
      "correct_tok c\n",
      "predicted_tok  construction\n",
      "correct_tok aud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  46%|████▌     | 181/396 [00:05<00:06, 34.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  woman\n",
      "correct_tok const\n",
      "predicted_tok  writer\n",
      "correct_tok an\n",
      "predicted_tok  writer\n",
      "correct_tok writer\n",
      "predicted_tok  assistant\n",
      "correct_tok guard\n",
      "predicted_tok  guard\n",
      "correct_tok ass\n",
      "predicted_tok  patient\n",
      "correct_tok phys\n",
      "predicted_tok  patient\n",
      "correct_tok att\n",
      "predicted_tok  accountant\n",
      "correct_tok lab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  48%|████▊     | 189/396 [00:05<00:06, 34.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  accountant\n",
      "correct_tok account\n",
      "predicted_tok  editor\n",
      "correct_tok an\n",
      "predicted_tok  editor\n",
      "correct_tok editor\n",
      "predicted_tok  sales\n",
      "correct_tok re\n",
      "predicted_tok  sales\n",
      "correct_tok s\n",
      "predicted_tok  teacher\n",
      "correct_tok te\n",
      "predicted_tok  teacher\n",
      "correct_tok super\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  50%|████▉     | 197/396 [00:06<00:05, 34.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  patient\n",
      "correct_tok phys\n",
      "predicted_tok  physician\n",
      "correct_tok cl\n",
      "predicted_tok  nurse\n",
      "correct_tok chief\n",
      "predicted_tok  nurse\n",
      "correct_tok n\n",
      "predicted_tok  accountant\n",
      "correct_tok an\n",
      "predicted_tok  accountant\n",
      "correct_tok account\n",
      "predicted_tok  woman\n",
      "correct_tok const\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  51%|█████     | 201/396 [00:06<00:05, 34.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  construction\n",
      "correct_tok re\n",
      "predicted_tok  ha\n",
      "correct_tok guard\n",
      "predicted_tok  guard\n",
      "correct_tok hair\n",
      "predicted_tok  developer\n",
      "correct_tok develop\n",
      "predicted_tok  accountant\n",
      "correct_tok account\n",
      "predicted_tok  counselor\n",
      "correct_tok manager\n",
      "predicted_tok  counselor\n",
      "correct_tok c\n",
      "predicted_tok  teacher\n",
      "correct_tok law\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  54%|█████▍    | 213/396 [00:06<00:05, 34.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  teacher\n",
      "correct_tok te\n",
      "predicted_tok  secretary\n",
      "correct_tok me\n",
      "predicted_tok  secretary\n",
      "correct_tok secret\n",
      "predicted_tok  baker\n",
      "correct_tok super\n",
      "predicted_tok  baker\n",
      "correct_tok b\n",
      "predicted_tok  jan\n",
      "correct_tok house\n",
      "predicted_tok  house\n",
      "correct_tok jan\n",
      "predicted_tok  counselor\n",
      "correct_tok super\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  55%|█████▍    | 217/396 [00:06<00:05, 34.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  supervisor\n",
      "correct_tok c\n",
      "predicted_tok  writer\n",
      "correct_tok CEO\n",
      "predicted_tok  CEO\n",
      "correct_tok writer\n",
      "predicted_tok  ha\n",
      "correct_tok me\n",
      "predicted_tok  ha\n",
      "correct_tok hair\n",
      "predicted_tok  cash\n",
      "correct_tok c\n",
      "predicted_tok  cash\n",
      "correct_tok cash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  57%|█████▋    | 225/396 [00:06<00:04, 34.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  auditor\n",
      "correct_tok s\n",
      "predicted_tok  auditor\n",
      "correct_tok aud\n",
      "predicted_tok  woman\n",
      "correct_tok c\n",
      "predicted_tok  car\n",
      "correct_tok b\n",
      "predicted_tok  designer\n",
      "correct_tok law\n",
      "predicted_tok  designer\n",
      "correct_tok design\n",
      "predicted_tok  accountant\n",
      "correct_tok me\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  59%|█████▉    | 233/396 [00:07<00:04, 34.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  accountant\n",
      "correct_tok account\n",
      "predicted_tok  cash\n",
      "correct_tok cook\n",
      "predicted_tok  customer\n",
      "correct_tok cash\n",
      "predicted_tok  cleaner\n",
      "correct_tok lab\n",
      "predicted_tok  lab\n",
      "correct_tok clean\n",
      "predicted_tok  teacher\n",
      "correct_tok me\n",
      "predicted_tok  teacher\n",
      "correct_tok te\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  61%|██████    | 241/396 [00:07<00:04, 33.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  baker\n",
      "correct_tok guard\n",
      "predicted_tok  baker\n",
      "correct_tok b\n",
      "predicted_tok  client\n",
      "correct_tok guard\n",
      "predicted_tok  tailor\n",
      "correct_tok tail\n",
      "predicted_tok  clerk\n",
      "correct_tok an\n",
      "predicted_tok  analyst\n",
      "correct_tok cl\n",
      "predicted_tok  woman\n",
      "correct_tok law\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  63%|██████▎   | 249/396 [00:07<00:04, 34.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  nurse\n",
      "correct_tok n\n",
      "predicted_tok  house\n",
      "correct_tok c\n",
      "predicted_tok  car\n",
      "correct_tok house\n",
      "predicted_tok  house\n",
      "correct_tok s\n",
      "predicted_tok  house\n",
      "correct_tok house\n",
      "predicted_tok  woman\n",
      "correct_tok phys\n",
      "predicted_tok  reception\n",
      "correct_tok re\n",
      "predicted_tok  editor\n",
      "correct_tok law\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  64%|██████▍   | 253/396 [00:07<00:04, 34.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  editor\n",
      "correct_tok editor\n",
      "predicted_tok  farmer\n",
      "correct_tok far\n",
      "predicted_tok  farmer\n",
      "correct_tok writer\n",
      "predicted_tok  l\n",
      "correct_tok s\n",
      "predicted_tok  l\n",
      "correct_tok l\n",
      "predicted_tok  teacher\n",
      "correct_tok s\n",
      "predicted_tok  sales\n",
      "correct_tok te\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  66%|██████▌   | 261/396 [00:08<00:03, 34.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  baker\n",
      "correct_tok jan\n",
      "predicted_tok  baker\n",
      "correct_tok b\n",
      "predicted_tok  mother\n",
      "correct_tok cook\n",
      "predicted_tok  cook\n",
      "correct_tok n\n",
      "predicted_tok  house\n",
      "correct_tok develop\n",
      "predicted_tok  developer\n",
      "correct_tok house\n",
      "predicted_tok  driver\n",
      "correct_tok driver\n",
      "predicted_tok  driver\n",
      "correct_tok account\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  68%|██████▊   | 269/396 [00:08<00:03, 34.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  woman\n",
      "correct_tok manager\n",
      "predicted_tok  manager\n",
      "correct_tok design\n",
      "predicted_tok  counselor\n",
      "correct_tok me\n",
      "predicted_tok  mechanic\n",
      "correct_tok c\n",
      "predicted_tok  cleaner\n",
      "correct_tok me\n",
      "predicted_tok  mechanic\n",
      "correct_tok clean\n",
      "predicted_tok  m\n",
      "correct_tok m\n",
      "predicted_tok  writer\n",
      "correct_tok writer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  70%|██████▉   | 277/396 [00:08<00:03, 34.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  tailor\n",
      "correct_tok c\n",
      "predicted_tok  tailor\n",
      "correct_tok tail\n",
      "predicted_tok  assistant\n",
      "correct_tok const\n",
      "predicted_tok  assistant\n",
      "correct_tok ass\n",
      "predicted_tok  secretary\n",
      "correct_tok super\n",
      "predicted_tok  supervisor\n",
      "correct_tok secret\n",
      "predicted_tok  chief\n",
      "correct_tok chief\n",
      "predicted_tok  chief\n",
      "correct_tok re\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  72%|███████▏  | 285/396 [00:08<00:03, 34.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  jan\n",
      "correct_tok jan\n",
      "predicted_tok  jan\n",
      "correct_tok aud\n",
      "predicted_tok  interviewer\n",
      "correct_tok an\n",
      "predicted_tok  tailor\n",
      "correct_tok tail\n",
      "predicted_tok  nurse\n",
      "correct_tok s\n",
      "predicted_tok  nurse\n",
      "correct_tok n\n",
      "predicted_tok  woman\n",
      "correct_tok jan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  74%|███████▍  | 293/396 [00:08<00:02, 34.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  jan\n",
      "correct_tok re\n",
      "predicted_tok  woman\n",
      "correct_tok m\n",
      "predicted_tok  tailor\n",
      "correct_tok tail\n",
      "predicted_tok  nurse\n",
      "correct_tok develop\n",
      "predicted_tok  nurse\n",
      "correct_tok n\n",
      "predicted_tok  baker\n",
      "correct_tok me\n",
      "predicted_tok  mechanic\n",
      "correct_tok b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  76%|███████▌  | 301/396 [00:09<00:02, 34.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  editor\n",
      "correct_tok editor\n",
      "predicted_tok  fact\n",
      "correct_tok s\n",
      "predicted_tok  m\n",
      "correct_tok c\n",
      "predicted_tok  m\n",
      "correct_tok m\n",
      "predicted_tok  supervisor\n",
      "correct_tok super\n",
      "predicted_tok  supervisor\n",
      "correct_tok clean\n",
      "predicted_tok  tailor\n",
      "correct_tok tail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  77%|███████▋  | 305/396 [00:09<00:02, 34.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  tailor\n",
      "correct_tok law\n",
      "predicted_tok  secretary\n",
      "correct_tok cook\n",
      "predicted_tok  boss\n",
      "correct_tok secret\n",
      "predicted_tok  secretary\n",
      "correct_tok secret\n",
      "predicted_tok  farmer\n",
      "correct_tok far\n",
      "predicted_tok  assistant\n",
      "correct_tok far\n",
      "predicted_tok  farmer\n",
      "correct_tok ass\n",
      "predicted_tok  supervisor\n",
      "correct_tok super\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating coreference:  80%|███████▉  | 315/396 [00:09<00:02, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_tok  supervisor\n",
      "correct_tok att\n",
      "predicted_tok  l\n",
      "correct_tok manager\n",
      "predicted_tok  l\n",
      "correct_tok l\n",
      "predicted_tok  patient\n",
      "correct_tok m\n",
      "predicted_tok  patient\n",
      "correct_tok n\n",
      "predicted_tok  woman\n",
      "correct_tok far\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/evaluate_model.py:166\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mrun_evaluation_on_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/evaluate_model.py:45\u001b[39m, in \u001b[36mrun_evaluation_on_task\u001b[39m\u001b[34m(model, tokenizer, model_name, task, test_file, output_dir)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m evaluator.save_results(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/evaluation/coreference.py:61\u001b[39m, in \u001b[36mEvaluateCoreference.evaluate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m test_example \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m.test_examples, \u001b[33m\"\u001b[39m\u001b[33mEvaluating coreference\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     59\u001b[39m     total[test_example[\u001b[33m\"\u001b[39m\u001b[33mgender\u001b[39m\u001b[33m\"\u001b[39m]] += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     probabilities = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_prediction_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_example\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     predicted_tok = \u001b[38;5;28mself\u001b[39m.tok.decode([probabilities.index(\u001b[38;5;28mmax\u001b[39m(probabilities))])\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpredicted_tok\u001b[39m\u001b[33m\"\u001b[39m, predicted_tok)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/project/pi_wenlongzhao_umass_edu/1/jkarnuthala/notebooks/temp/DAMA/src/evaluation/evaluate.py:31\u001b[39m, in \u001b[36mEvaluate.get_prediction_probability\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m     29\u001b[39m device = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.model.parameters()).device\n\u001b[32m     30\u001b[39m input_ids = input_ids.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].float()\n\u001b[32m     32\u001b[39m probabilities = logits.softmax(dim=\u001b[32m2\u001b[39m)[:,-\u001b[32m1\u001b[39m,:].squeeze()\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m probabilities.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    910\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    911\u001b[39m         block.\u001b[34m__call__\u001b[39m,\n\u001b[32m    912\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    919\u001b[39m         output_attentions,\n\u001b[32m    920\u001b[39m     )\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:440\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[39m\n\u001b[32m    437\u001b[39m     outputs = outputs + cross_attn_outputs[\u001b[32m2\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m    439\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m feed_forward_hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dama/lib/python3.11/site-packages/torch/nn/functional.py:2910\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2900\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2902\u001b[39m         layer_norm,\n\u001b[32m   2903\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2908\u001b[39m         eps=eps,\n\u001b[32m   2909\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2910\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2911\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2912\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%run evaluate_model.py \\\n",
    "    --param_number 7 \\\n",
    "    --method \"ALPHA_EDIT\" \\\n",
    "    --test_file \"anti_type1_test.txt\" \\\n",
    "    --test_task \"coref\" \\\n",
    "    --model_name gpt2-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
